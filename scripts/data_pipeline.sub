# CHTC maintained container for AlphaFold3 as of December 2025
# Can use the local CHTC copy at file:///staging/groups/chtc_staff/containers/alphafold3.minimal.22Jan2025.sif
container_image = osdf:///osg-public/containers/alphafold3.minimal.22Jan2025.sif

executable = scripts/data_pipeline.sh

log = ./logs/data_pipeline.log
output = data_pipeline_$(Cluster)_$(Process).out
error  = data_pipeline_$(Cluster)_$(Process).err

initialdir = $(my_directory)
transfer_input_files = data_inputs/

# transfer output files back to the submit node
transfer_output_files = $(my_directory).data_pipeline.tar.gz
transfer_output_remaps = "$(my_directory).data_pipeline.tar.gz=inference_inputs/$(my_directory).data_pipeline.tar.gz"

should_transfer_files = YES
when_to_transfer_output = ON_EXIT

# We need this to transfer the databases to the execute node
Requirements = (Target.HasCHTCStaging == true) && (TARGET.HasAlphafold3 == true)

if defined USE_SMALL_DB
  # testing requirements
  request_memory = 8GB
  request_disk = 16GB
  request_cpus = 4
  arguments = --smalldb --work_dir_ext $(Cluster)_$(Process) --verbose
else
  # full requirements
  request_memory = 8GB
  # Request less disk if matched machine already has AF3 DB preloaded (650GB savings)
  request_disk = 700000 - ( (TARGET.HasAlphafold3?: 1) * 650000)
  request_cpus = 8
  arguments = --work_dir_ext $(Cluster)_$(Proc)
endif

queue my_directory from list_of_af3_jobs.txt